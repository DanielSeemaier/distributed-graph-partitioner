#!/bin/bash 
. "$script_pwd/../systems/generic"

_ExecRsyncCommand() {
    if command -v sshpass; then
        if [[ ${SUPERMUC_PASS:-""} != "" ]]; then 
            sshpass -p "$SUPERMUC_PASS" $cmd
        else
            eval $cmd
        fi
    else
        eval $cmd
    fi
}

UploadExperiment() {
    if [[ "${_username}" == "" ]]; then
        echo "Error: provide username via the Username command, e.g., Username skx1234"
        exit 1
    fi

    dir="$(pwd)"
    name=${dir##*/}

    cmd='rsync -rutvP --cvs-exclude . '$_username'@skx.supermuc.lrz.de:~/'$name
    _ExecRsyncCommand "$cmd"
}

DownloadExperiment() {
    if [[ "${_username}" == "" ]]; then
        echo "Error: provide username via the Username command, e.g., Username skx1234"
        exit 1
    fi

    dir="$(pwd)"
    name=${dir##*/}

    cmd='rsync -rutvP --cvs-exclude '$_username'@skx.supermuc.lrz.de:~/'$name'/ .'
    _ExecRsyncCommand "$cmd"
}

SetupBuildEnv() {
    module restore mkexp-supermuc
    #if [[ "$_mpi" == "OpenMPI" ]]; then 
    #    module load openmpi/${SUPERMUC_OPENMPI_VERSION}
    #elif [[ "$_mpi" == "IMPI" ]]; then
    #    module load intel-oneapi-mpi/${SUPERMUC_IMPI_VERSION}
    #fi
}

GenerateJobfileHeader() {
    local -n args=$1

    if (( $((args[num_mpis] * args[num_threads])) > 48 )); then 
        >&2 echo "Error: too many MPI processes * threads"
        exit 1
    fi
    if [[ $_project == "" ]]; then
        >&2 echo "Error: no project specified"
        exit 1
    fi

    echo "#!/bin/bash"
    echo "#SBATCH --nodes=${args[num_nodes]}"
    echo "#SBATCH --ntasks=$((args[num_nodes]*args[num_mpis]))"
    echo "#SBATCH --cpus-per-task=${args[num_threads]}"
    echo "#SBATCH --ntasks-per-node=${args[num_mpis]}"
    echo "#SBATCH --switches=1"
    echo "#SBATCH --ear=off"
    echo "#SBATCH --account=$_project"
    echo "#SBATCH --time=${args[timelimit]}"

    if [[ "$_partition" == "" ]]; then 
        if (( ${args[num_nodes]} < 17 )); then
            partition="micro"
        elif (( ${args[num_nodes]} < 769 )); then
            partition="general"
        else
            partition="large"
        fi
    else 
        partition="$_partition"
    fi
    echo "#SBATCH --partition=$partition"

    echo "module restore mkexp-supermuc"
    #if [[ "${args[mpi]}" == "OpenMPI" ]]; then 
    #    echo "module load openmpi/${SUPERMUC_OPENMPI_VERSION}"
    #elif [[ "${args[mpi]}" == "IMPI" ]]; then
    #    echo "module load intel-oneapi-mpi/${SUPERMUC_IMPI_VERSION}"
    #fi

    if [[ "${args[mpi]}" != "IMPI" ]]; then
        >&2 echo "Error: application must be run with IMPI"
        exit 1
    fi

    echo "unset OMP_NUM_THREADS"
    echo "unset OMP_PROC_BIND"
    echo "unset OMP_PLACES"
    
    echo "export I_MPI_PIN_CELL=core"
    echo "export I_MPI_PIN_DOMAIN=${args[num_threads]}:compact"
    if [[ "${args[timeout]}" != "0" ]]; then 
        echo "export I_MPI_JOB_TIMEOUT=${args[timeout]}"
    fi
}

GenerateJobfileEntry() {
    local -n args=$1

    if [[ "${args[print_wrapper]}" == "1" ]]; then
        >&2 echo -e "Wrapping calls with ${args[mpi]} for algorithm '$ALGO_COLOR${args[algorithm]}$NO_COLOR', $ARGS_COLOR${args[num_nodes]}x${args[num_mpis]}x${args[num_threads]}$NO_COLOR:"
    fi

    case "${args[mpi]}" in
        none)
            >&2 echo "Error: application must be run with MPI"
            exit 1
            ;;
        #OpenMPI)
        #    echo "mpirun -n $((args[num_nodes]*args[num_mpis])) --bind-to core --map-by socket:PE=${args[num_threads]} ${args[exe]}"
        #    ;;
        IMPI)
            echo "mpiexec -n $((args[num_nodes]*args[num_mpis])) --perhost ${args[num_mpis]} ${args[exe]}"
            ;;
        *)
            >&2 echo "Error: unsupported MPI ${args[mpi]}"
            exit 1
    esac
}

GenerateJobfileSubmission() {
    for jobfile in ${@}; do 
        echo "sbatch $jobfile"
    done
}

