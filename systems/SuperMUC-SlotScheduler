#!/bin/bash 
. "$script_pwd/../systems/Generic"

_ExecRsyncCommand() {
    Prefixed eval $cmd
}

UploadDirectory() {
    local dir="$1"
    local name="$2"

    if [[ "${_username}" == "" ]]; then
        echo "Error: provide username via the Username command, e.g., Username skx1234"
        exit 1
    fi

    cmd='rsync -rutvP --cvs-exclude '"$dir"' '$_username'@skx.supermuc.lrz.de:~/'"$name"
    _ExecRsyncCommand "$cmd"
}

DownloadDirectory() {
    local dir="$1"
    local name="$2"

    if [[ "${_username}" == "" ]]; then
        echo "Error: provide username via the Username command, e.g., Username skx1234"
        exit 1
    fi

    cmd='rsync -rutvP --cvs-exclude '$_username'@skx.supermuc.lrz.de:~/'$name'/ '"$dir"
    _ExecRsyncCommand "$cmd"
}

SetupBuildEnv() {
    Prefixed module restore "$ROOT/data/SuperMUC/modules-scheduler"
}

SetupSystem() {
    return 0
}

WORKER_HEADER_FILENAME="worker.tmp"
WORKER_FILENAME="worker.sh"
SCHEDULER_HEADER_FILENAME="scheduler.tmp"
SCHEDULER_FILENAME="scheduler.py"
GLOBAL_QUEUE_FILENAME="global_queue.txt"

GenerateJobfileHeader() {
    local -n args=$1

    if (( $((args[num_mpis] * args[num_threads])) > 48 )); then 
        >&2 echo "Error: too many MPI processes * threads"
        exit 1
    fi
    if [[ $_project == "" ]]; then
        >&2 echo "Error: no project specified"
        exit 1
    fi
    if [[ "${args[mpi]}" != "IMPI" ]]; then
        >&2 echo "Error: application must be run with IMPI"
        exit 1
    fi


    echo "#!/bin/bash" > $WORKER_HEADER_FILENAME
    echo "#SBATCH --nodes=1" >> $WORKER_HEADER_FILENAME
    echo "#SBATCH --ntasks=1" >> $WORKER_HEADER_FILENAME
    echo "#SBATCH --cpus-per-task=${args[num_threads]}" >> $WORKER_HEADER_FILENAME
    echo "#SBATCH --ntasks-per-node=1" >> $WORKER_HEADER_FILENAME
    echo "#SBATCH --switches=1" >> $WORKER_HEADER_FILENAME
    echo "#SBATCH --ear=off" >> $WORKER_HEADER_FILENAME
    echo "#SBATCH --account=$_project" >> $WORKER_HEADER_FILENAME
    echo "#SBATCH --time=${args[timelimit]}" >> $WORKER_HEADER_FILENAME

    if [[ "$_partition" == "" ]]; then 
        partition="general"
    else 
        partition="$_partition"
    fi
    echo "#SBATCH --partition=$partition" >> $WORKER_HEADER_FILENAME

    echo "module restore $ROOT/data/SuperMUC/modules-scheduler" >> $WORKER_HEADER_FILENAME
    echo "unset OMP_NUM_THREADS" >> $WORKER_HEADER_FILENAME
    echo "unset OMP_PROC_BIND" >> $WORKER_HEADER_FILENAME
    echo "unset OMP_PLACES" >> $WORKER_HEADER_FILENAME
    echo "export I_MPI_PIN_CELL=core" >> $WORKER_HEADER_FILENAME
    echo "export I_MPI_PIN_DOMAIN=${args[num_threads]}:compact" >> $WORKER_HEADER_FILENAME
    if [[ "${args[timeout]}" != "0" ]]; then  >> $WORKER_HEADER_FILENAME
        echo "export I_MPI_JOB_TIMEOUT=${args[timeout]}" >> $WORKER_HEADER_FILENAME
    fi

    echo "#!/usr/bin/env python3" > $SCHEDULER_HEADER_FILENAME
    echo "MAX_TASKS_IN_SLOT_QUEUE=6" >> $SCHEDULER_HEADER_FILENAME
    echo "MAX_JOBS_IN_QUEUE=${args[num_nodes]}" >> $SCHEDULER_HEADER_FILENAME

    echo ""
}

GenerateJobfileEntry() {
    local -n args=$1

    if [[ "${args[print_wrapper]}" == "1" ]]; then
        >&2 echo -e "Wrapping calls with ${args[mpi]} for algorithm '$ALGO_COLOR${args[algorithm]}$NO_COLOR', $ARGS_COLOR${args[num_nodes]}x${args[num_mpis]}x${args[num_threads]}$NO_COLOR:"
    fi

    case "${args[mpi]}" in
        none)
            >&2 echo "Error: application must be run with MPI"
            exit 1
            ;;
        IMPI)
            echo "mpiexec -n ${args[num_mpis]} --perhost ${args[num_mpis]} ${args[exe]}"
            ;;
        *)
            >&2 echo "Error: unsupported MPI ${args[mpi]}"
            exit 1
    esac
}

GenerateJobfileSubmission() {
    cat $SCHEDULER_HEADER_FILENAME > $SCHEDULER_FILENAME
    cat "$ROOT/data/SuperMUC/scheduler/scheduler.py" >> "scheduler.py"

    cat $WORKER_HEADER_FILENAME > $WORKER_FILENAME 
    cat "$ROOT/data/SuperMUC/scheduler/worker.sh" >> $WORKER_FILENAME

    echo "" > $GLOBAL_QUEUE_FILENAME
    for jobfile in ${@}; do 
        cat $jobfile >> $GLOBAL_QUEUE_FILENAME
    done

    echo "python $SCHEDULER_FILENAME $GLOBAL_QUEUE_FILENAME"
}

